{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "#from spacy.training import Scorer\n",
    "from spacy.training import Example\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "import fitz  # PyMuPDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with selected entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            if valid_start < valid_end:\n",
    "                valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath, selected_entities):\n",
    "    try:\n",
    "        training_data = []\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    # Filter out only \"Skills\" and \"Name\" labels\n",
    "                    if label in selected_entities:\n",
    "                        entities.append((point['start'], point['end'] + 1, label))\n",
    "\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def keep_longest_entities(entities):\n",
    "    \"\"\"Remove overlapping entities by keeping only the longest one\"\"\"\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "\n",
    "    non_overlapping_entities = []\n",
    "    \n",
    "    for i, (start, end, label) in enumerate(entities):\n",
    "        if i == 0:\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "        else:\n",
    "            prev_start, prev_end, prev_label = non_overlapping_entities[-1]\n",
    "            if start >= prev_end:\n",
    "                non_overlapping_entities.append((start, end, label))\n",
    "            else:\n",
    "                if (end - start) > (prev_end - prev_start):\n",
    "                    non_overlapping_entities[-1] = (start, end, label)\n",
    "\n",
    "    return non_overlapping_entities\n",
    "\n",
    "\n",
    "def train_spacy(selected_entities):\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"./data/traindata.json\",selected_entities)\n",
    "    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner', last=True)\n",
    "\n",
    "    # Add only \"Skills\" and \"Name\" labels to the NER component\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        annotations['entities'] = keep_longest_entities(annotations['entities'])\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])  # Only adds \"Skills\" and \"Name\" labels\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # Only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(30):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    drop=0.3,\n",
    "                    sgd=optimizer,\n",
    "                    losses=losses\n",
    "                )\n",
    "            print(losses)\n",
    "\n",
    "    return losses, nlp\n",
    "\n",
    "# Train the model\n",
    "#'College Name', 'Companies worked at', 'Degree', 'Designation', 'Email Address', 'Graduation Year', 'Location', 'Name', 'Skills', 'UNKNOWN', 'Years of Experience'\n",
    "selected_entities = [\"Skills\", \"Name\"]\n",
    "losses, nlp = train_spacy(selected_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline components: ['ner']\n",
      "NER labels: ('Name', 'Skills')\n",
      "NER configuration: {'moves': None, 'update_with_oracle_cut_size': 100, 'multitasks': [], 'min_action_freq': 1, 'learn_tokens': False, 'beam_width': 1, 'beam_density': 0.0, 'beam_update_prob': 0.0, 'incorrect_spans_key': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline components:\", nlp.pipe_names)\n",
    "# Accessing the NER component parameters\n",
    "ner = nlp.get_pipe('ner')\n",
    "print(\"NER labels:\", ner.labels)\n",
    "\n",
    "# You can also inspect the model's config settings\n",
    "print(\"NER configuration:\", ner.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': {'train': None, 'dev': None, 'vectors': None, 'init_tok2vec': None},\n",
       " 'system': {'seed': 0, 'gpu_allocator': None},\n",
       " 'nlp': {'lang': 'en',\n",
       "  'pipeline': ['ner'],\n",
       "  'disabled': [],\n",
       "  'before_creation': None,\n",
       "  'after_creation': None,\n",
       "  'after_pipeline_creation': None,\n",
       "  'batch_size': 1000,\n",
       "  'tokenizer': {'@tokenizers': 'spacy.Tokenizer.v1'},\n",
       "  'vectors': {'@vectors': 'spacy.Vectors.v1'}},\n",
       " 'components': {'ner': {'factory': 'ner',\n",
       "   'incorrect_spans_key': None,\n",
       "   'model': {'@architectures': 'spacy.TransitionBasedParser.v2',\n",
       "    'state_type': 'ner',\n",
       "    'extra_state_tokens': False,\n",
       "    'hidden_width': 64,\n",
       "    'maxout_pieces': 2,\n",
       "    'use_upper': True,\n",
       "    'nO': None,\n",
       "    'tok2vec': {'@architectures': 'spacy.HashEmbedCNN.v2',\n",
       "     'pretrained_vectors': None,\n",
       "     'width': 96,\n",
       "     'depth': 4,\n",
       "     'embed_size': 2000,\n",
       "     'window_size': 1,\n",
       "     'maxout_pieces': 3,\n",
       "     'subword_features': True}},\n",
       "   'moves': None,\n",
       "   'scorer': {'@scorers': 'spacy.ner_scorer.v1'},\n",
       "   'update_with_oracle_cut_size': 100}},\n",
       " 'corpora': {'dev': {'@readers': 'spacy.Corpus.v1',\n",
       "   'path': '${paths.dev}',\n",
       "   'gold_preproc': False,\n",
       "   'max_length': 0,\n",
       "   'limit': 0,\n",
       "   'augmenter': None},\n",
       "  'train': {'@readers': 'spacy.Corpus.v1',\n",
       "   'path': '${paths.train}',\n",
       "   'gold_preproc': False,\n",
       "   'max_length': 0,\n",
       "   'limit': 0,\n",
       "   'augmenter': None}},\n",
       " 'training': {'seed': '${system.seed}',\n",
       "  'gpu_allocator': '${system.gpu_allocator}',\n",
       "  'dropout': 0.1,\n",
       "  'accumulate_gradient': 1,\n",
       "  'patience': 1600,\n",
       "  'max_epochs': 0,\n",
       "  'max_steps': 20000,\n",
       "  'eval_frequency': 200,\n",
       "  'frozen_components': [],\n",
       "  'annotating_components': [],\n",
       "  'dev_corpus': 'corpora.dev',\n",
       "  'train_corpus': 'corpora.train',\n",
       "  'before_to_disk': None,\n",
       "  'before_update': None,\n",
       "  'batcher': {'@batchers': 'spacy.batch_by_words.v1',\n",
       "   'discard_oversize': False,\n",
       "   'tolerance': 0.2,\n",
       "   'get_length': None,\n",
       "   'size': {'@schedules': 'compounding.v1',\n",
       "    'start': 100,\n",
       "    'stop': 1000,\n",
       "    'compound': 1.001,\n",
       "    't': 0.0}},\n",
       "  'logger': {'@loggers': 'spacy.ConsoleLogger.v1', 'progress_bar': False},\n",
       "  'optimizer': {'@optimizers': 'Adam.v1',\n",
       "   'beta1': 0.9,\n",
       "   'beta2': 0.999,\n",
       "   'L2_is_weight_decay': True,\n",
       "   'L2': 0.01,\n",
       "   'grad_clip': 1.0,\n",
       "   'use_averages': False,\n",
       "   'eps': 1e-08,\n",
       "   'learn_rate': 0.001},\n",
       "  'score_weights': {'ents_f': 1.0,\n",
       "   'ents_p': 0.0,\n",
       "   'ents_r': 0.0,\n",
       "   'ents_per_type': None}},\n",
       " 'pretraining': {},\n",
       " 'initialize': {'vectors': '${paths.vectors}',\n",
       "  'init_tok2vec': '${paths.init_tok2vec}',\n",
       "  'vocab_data': None,\n",
       "  'lookups': None,\n",
       "  'before_init': None,\n",
       "  'after_init': None,\n",
       "  'components': {},\n",
       "  'tokenizer': {}}}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<thinc.model.Model object at 0x7dc9c712b040>, <thinc.model.Model object at 0x7dc9c712b0c0>, <thinc.model.Model object at 0x7dc9c712af40>]\n"
     ]
    }
   ],
   "source": [
    "# Get the tok2vec component (if using a CNN-based model)\n",
    "tok2vec = nlp.get_pipe(\"ner\").model\n",
    "\n",
    "# Print the architecture of the token-to-vector embedding\n",
    "print(tok2vec.layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model/NER_NLP_it10\n"
     ]
    }
   ],
   "source": [
    "# save the model for n interation\n",
    "# spaCy model and it's stored in `nlp`\n",
    "output_dir = \"./model/NER_NLP_it10\"\n",
    "# Save the trained model to the output directory\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model/NER_NLP_pretrain_ts\n"
     ]
    }
   ],
   "source": [
    "# spaCy model and it's stored in `nlp`\n",
    "output_dir = \"./model/NER_NLP_pretrain_ts\"\n",
    "# Save the trained model to the output directory\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./model/NER_NLP_it15\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "output_dir = \"./model/NER_NLP_it15\"\n",
    "nlp_15 = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER training with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Angad Waghmare\n",
      "Pune, Maharashtra - Email me on Ind...\" with entities \"[(0, 14, 'Name'), (15, 19, 'Location'), (55, 99, '...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"B. Gokul\n",
      "Gokul, Uttar Pradesh - Email me on Indeed...\" with entities \"[(0, 8, 'Name'), (9, 14, 'Location'), (52, 89, 'Em...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Srinivas VO\n",
      "Sr. Test Manager\n",
      "\n",
      "Mumbai, Maharashtra ...\" with entities \"[(0, 11, 'Name'), (12, 28, 'Designation'), (30, 36...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Jatin Arora\n",
      "SDET Automation Engineer, Infosys - CR...\" with entities \"[(0, 11, 'Name'), (12, 36, 'Designation'), (38, 45...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Puran Mal\n",
      "Jaipur, Rajasthan - Email me on Indeed: ...\" with entities \"[(0, 9, 'Name'), (10, 16, 'Location'), (50, 89, 'E...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ravi Shankar\n",
      "Working as Escalation Engineer with M...\" with entities \"[(0, 12, 'Name'), (13, 43, 'Designation'), (49, 58...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Pavithra M\n",
      "\"Infosys\" internship\n",
      "\n",
      "Bengaluru, Karnat...\" with entities \"[(0, 10, 'Name'), (12, 19, 'Companies worked at'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Senthil Kumar\n",
      "Senior Technical Lead - HCL Cisco\n",
      "\n",
      "-...\" with entities \"[(0, 13, 'Name'), (14, 35, 'Designation'), (38, 41...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dilliraja Baskaran\n",
      "Tamil Nadu - Email me on Indeed...\" with entities \"[(0, 17, 'Name'), (19, 29, 'Location'), (52, 100, ...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      "Training Loss: {'ner': 9667.092483090895}\n",
      "Validation Loss: {'ner': 2318.2839959272533}\n",
      "Starting iteration 1\n",
      "Iteration 1:\n",
      "Training Loss: {'ner': 4549.441670670907}\n",
      "Validation Loss: {'ner': 992.8351454485372}\n",
      "Starting iteration 2\n",
      "Iteration 2:\n",
      "Training Loss: {'ner': 3430.299362034069}\n",
      "Validation Loss: {'ner': 774.7777631536816}\n",
      "Starting iteration 3\n",
      "Iteration 3:\n",
      "Training Loss: {'ner': 2888.9658816551405}\n",
      "Validation Loss: {'ner': 785.1456278590722}\n",
      "Starting iteration 4\n",
      "Iteration 4:\n",
      "Training Loss: {'ner': 2794.1735148918715}\n",
      "Validation Loss: {'ner': 618.6892733708938}\n",
      "Starting iteration 5\n",
      "Iteration 5:\n",
      "Training Loss: {'ner': 2377.453926946979}\n",
      "Validation Loss: {'ner': 539.2693904952678}\n",
      "Starting iteration 6\n",
      "Iteration 6:\n",
      "Training Loss: {'ner': 2246.913681573162}\n",
      "Validation Loss: {'ner': 498.9449821970614}\n",
      "Starting iteration 7\n",
      "Iteration 7:\n",
      "Training Loss: {'ner': 2064.8253013411545}\n",
      "Validation Loss: {'ner': 511.19238975670373}\n",
      "Starting iteration 8\n",
      "Iteration 8:\n",
      "Training Loss: {'ner': 1972.2647664645028}\n",
      "Validation Loss: {'ner': 442.93868072724047}\n",
      "Starting iteration 9\n",
      "Iteration 9:\n",
      "Training Loss: {'ner': 1904.6094414095921}\n",
      "Validation Loss: {'ner': 414.36605207856365}\n",
      "Starting iteration 10\n",
      "Iteration 10:\n",
      "Training Loss: {'ner': 1836.8362881127864}\n",
      "Validation Loss: {'ner': 368.00199001899165}\n",
      "Starting iteration 11\n",
      "Iteration 11:\n",
      "Training Loss: {'ner': 1745.8826129770544}\n",
      "Validation Loss: {'ner': 348.0062688583075}\n",
      "Starting iteration 12\n",
      "Iteration 12:\n",
      "Training Loss: {'ner': 1686.5970395285995}\n",
      "Validation Loss: {'ner': 335.50963399402326}\n",
      "Starting iteration 13\n",
      "Iteration 13:\n",
      "Training Loss: {'ner': 1559.3901146205449}\n",
      "Validation Loss: {'ner': 329.22953005798854}\n",
      "Starting iteration 14\n",
      "Iteration 14:\n",
      "Training Loss: {'ner': 1540.5880790767906}\n",
      "Validation Loss: {'ner': 291.3657892019502}\n",
      "Starting iteration 15\n",
      "Iteration 15:\n",
      "Training Loss: {'ner': 1509.1348463484564}\n",
      "Validation Loss: {'ner': 269.5351234695619}\n",
      "Starting iteration 16\n",
      "Iteration 16:\n",
      "Training Loss: {'ner': 1407.8237082277521}\n",
      "Validation Loss: {'ner': 255.51821924041602}\n",
      "Starting iteration 17\n",
      "Iteration 17:\n",
      "Training Loss: {'ner': 1382.0622711296617}\n",
      "Validation Loss: {'ner': 239.12637450953136}\n",
      "Starting iteration 18\n",
      "Iteration 18:\n",
      "Training Loss: {'ner': 1394.1980509518417}\n",
      "Validation Loss: {'ner': 234.61265907921063}\n",
      "Starting iteration 19\n",
      "Iteration 19:\n",
      "Training Loss: {'ner': 1278.7816458917418}\n",
      "Validation Loss: {'ner': 220.34745441758812}\n",
      "Starting iteration 20\n",
      "Iteration 20:\n",
      "Training Loss: {'ner': 1331.6678593172633}\n",
      "Validation Loss: {'ner': 239.62759439968275}\n",
      "Starting iteration 21\n",
      "Iteration 21:\n",
      "Training Loss: {'ner': 1219.112316807614}\n",
      "Validation Loss: {'ner': 199.78010897581464}\n",
      "Starting iteration 22\n",
      "Iteration 22:\n",
      "Training Loss: {'ner': 1191.0317359237536}\n",
      "Validation Loss: {'ner': 165.05648731052838}\n",
      "Starting iteration 23\n",
      "Iteration 23:\n",
      "Training Loss: {'ner': 1096.511520489017}\n",
      "Validation Loss: {'ner': 193.18842456156653}\n",
      "Starting iteration 24\n",
      "Iteration 24:\n",
      "Training Loss: {'ner': 1111.1508575810496}\n",
      "Validation Loss: {'ner': 174.47837752091124}\n",
      "Starting iteration 25\n",
      "Iteration 25:\n",
      "Training Loss: {'ner': 1095.4772791829641}\n",
      "Validation Loss: {'ner': 171.68949301110857}\n",
      "Starting iteration 26\n",
      "Iteration 26:\n",
      "Training Loss: {'ner': 1246.329956004848}\n",
      "Validation Loss: {'ner': 173.5606600574754}\n",
      "Starting iteration 27\n",
      "Iteration 27:\n",
      "Training Loss: {'ner': 1055.4491542904843}\n",
      "Validation Loss: {'ner': 153.28942571848185}\n",
      "Starting iteration 28\n",
      "Iteration 28:\n",
      "Training Loss: {'ner': 1041.9367719870545}\n",
      "Validation Loss: {'ner': 135.59294875427366}\n",
      "Starting iteration 29\n",
      "Iteration 29:\n",
      "Training Loss: {'ner': 976.4998892751887}\n",
      "Validation Loss: {'ner': 133.60024397884263}\n",
      "Starting iteration 30\n",
      "Iteration 30:\n",
      "Training Loss: {'ner': 1055.770660074648}\n",
      "Validation Loss: {'ner': 133.74918892113283}\n",
      "Starting iteration 31\n",
      "Iteration 31:\n",
      "Training Loss: {'ner': 926.4847041942885}\n",
      "Validation Loss: {'ner': 147.41036386533537}\n",
      "Starting iteration 32\n",
      "Iteration 32:\n",
      "Training Loss: {'ner': 954.6252746331343}\n",
      "Validation Loss: {'ner': 122.56828276609185}\n",
      "Starting iteration 33\n",
      "Iteration 33:\n",
      "Training Loss: {'ner': 1078.9422653753165}\n",
      "Validation Loss: {'ner': 134.7204100092658}\n",
      "Starting iteration 34\n",
      "Iteration 34:\n",
      "Training Loss: {'ner': 872.0310884306272}\n",
      "Validation Loss: {'ner': 128.64545684382088}\n",
      "Starting iteration 35\n",
      "Iteration 35:\n",
      "Training Loss: {'ner': 1003.1880278017185}\n",
      "Validation Loss: {'ner': 124.22658716062291}\n",
      "Starting iteration 36\n",
      "Iteration 36:\n",
      "Training Loss: {'ner': 883.702646917738}\n",
      "Validation Loss: {'ner': 134.4575824578134}\n",
      "Starting iteration 37\n",
      "Iteration 37:\n",
      "Training Loss: {'ner': 847.2991666089279}\n",
      "Validation Loss: {'ner': 114.17819891822715}\n",
      "Starting iteration 38\n",
      "Iteration 38:\n",
      "Training Loss: {'ner': 938.7171470638415}\n",
      "Validation Loss: {'ner': 123.34860518888868}\n",
      "Starting iteration 39\n",
      "Iteration 39:\n",
      "Training Loss: {'ner': 892.4206456328691}\n",
      "Validation Loss: {'ner': 110.50627362412288}\n",
      "Starting iteration 40\n",
      "Iteration 40:\n",
      "Training Loss: {'ner': 745.6937358465402}\n",
      "Validation Loss: {'ner': 103.4103554545923}\n",
      "Starting iteration 41\n",
      "Iteration 41:\n",
      "Training Loss: {'ner': 710.9597767217764}\n",
      "Validation Loss: {'ner': 95.35810125360213}\n",
      "Starting iteration 42\n",
      "Iteration 42:\n",
      "Training Loss: {'ner': 753.5222605496466}\n",
      "Validation Loss: {'ner': 97.16129546832398}\n",
      "Starting iteration 43\n",
      "Iteration 43:\n",
      "Training Loss: {'ner': 844.1695268109727}\n",
      "Validation Loss: {'ner': 102.22734572036985}\n",
      "Starting iteration 44\n",
      "Iteration 44:\n",
      "Training Loss: {'ner': 714.0028452106459}\n",
      "Validation Loss: {'ner': 101.3974285804635}\n",
      "Starting iteration 45\n",
      "Iteration 45:\n",
      "Training Loss: {'ner': 676.09624618042}\n",
      "Validation Loss: {'ner': 104.06336890345553}\n",
      "Starting iteration 46\n",
      "Iteration 46:\n",
      "Training Loss: {'ner': 742.0277343085709}\n",
      "Validation Loss: {'ner': 85.66601109832114}\n",
      "Starting iteration 47\n",
      "Iteration 47:\n",
      "Training Loss: {'ner': 695.2426151336514}\n",
      "Validation Loss: {'ner': 106.37776792127188}\n",
      "Starting iteration 48\n",
      "Iteration 48:\n",
      "Training Loss: {'ner': 676.0541300574279}\n",
      "Validation Loss: {'ner': 99.5720389769725}\n",
      "Starting iteration 49\n",
      "Iteration 49:\n",
      "Training Loss: {'ner': 691.626234429612}\n",
      "Validation Loss: {'ner': 118.03844236594867}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to split training data into train and validation sets\n",
    "def split_data(data, test_size=0.2):\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    return train_data, val_data\n",
    "\n",
    "# Function to calculate validation loss\n",
    "def evaluate_on_validation(nlp, val_data):\n",
    "    val_losses = {}\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], drop=0.0, losses=val_losses)\n",
    "    return val_losses\n",
    "\n",
    "def train_spacy():\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"../data/traindata.json\")\n",
    "    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n",
    "\n",
    "    # Split data into training and validation sets (80% train, 20% validation)\n",
    "    train_data, val_data = split_data(TRAIN_DATA, test_size=0.1)\n",
    "\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner', last=True)\n",
    "\n",
    "    # Add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        annotations['entities'] = keep_longest_entities(annotations['entities'])  # Keep only longest entities\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(50):\n",
    "            print(f\"Starting iteration {itn}\")\n",
    "            random.shuffle(train_data)\n",
    "            train_losses = {}\n",
    "            \n",
    "            # Training phase\n",
    "            for text, annotations in train_data:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], drop=0.2, sgd=optimizer, losses=train_losses)\n",
    "\n",
    "            # Validation phase (no training, just forward pass)\n",
    "            val_losses = evaluate_on_validation(nlp, val_data)\n",
    "            \n",
    "            # Print losses for both training and validation sets\n",
    "            print(f\"Iteration {itn}:\")\n",
    "            print(f\"Training Loss: {train_losses}\")\n",
    "            print(f\"Validation Loss: {val_losses}\")\n",
    "\n",
    "    return nlp, val_losses, train_losses\n",
    "\n",
    "# Train the model and return it\n",
    "trained_nlp, val_losses, train_losses = train_spacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./model/NER_NLP_sl2_it20_dp03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ananya Chavan\n",
      "lecturer - oracle tutorials\n",
      "\n",
      "Mumbai,...\" with entities \"[(0, 13, 'Name'), (973, 1703, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Name       0.95      0.97      0.96        39\n",
      "           O       0.95      0.99      0.97     12285\n",
      "      Skills       0.79      0.48      0.60      1184\n",
      "\n",
      "    accuracy                           0.94     13508\n",
      "   macro avg       0.90      0.82      0.84     13508\n",
      "weighted avg       0.94      0.94      0.94     13508\n",
      "\n",
      "Accuracy: 94.34%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(nlp, test_data, selected_entities=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for text, annotations in test_data:\n",
    "        doc = nlp(text)  # Apply the model to the test text\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)  # Create example using ground truth\n",
    "        \n",
    "        # Ensure the example's NER alignment is valid\n",
    "        aligned_ner = example.get_aligned_ner()\n",
    "        \n",
    "        if aligned_ner is None:\n",
    "            print(f\"Warning: NER alignment failed for text: {text}\")\n",
    "            continue\n",
    "        \n",
    "        # Create a list of token-based true labels for the text\n",
    "        true_labels = [\"O\"] * len(doc)  # Initialize with \"O\" (no entity)\n",
    "        \n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            for token in doc:\n",
    "                if token.idx >= start and token.idx + len(token) <= end:\n",
    "                    true_labels[token.i] = label\n",
    "        \n",
    "        # Create a list of token-based predicted labels\n",
    "        pred_labels = [\"O\"] * len(doc)\n",
    "        for ent in doc.ents:\n",
    "            for token in doc:\n",
    "                if token.idx >= ent.start_char and token.idx + len(token) <= ent.end_char:\n",
    "                    pred_labels[token.i] = ent.label_\n",
    "        \n",
    "        # Add the true and predicted labels to the global lists\n",
    "        y_true.extend(true_labels)\n",
    "        y_pred.extend(pred_labels)\n",
    "\n",
    "    # Filter y_true and y_pred to only include selected entities\n",
    "    if selected_entities is not None:\n",
    "        y_true_filtered = [\n",
    "            label if label in selected_entities or label == \"O\" else \"O\" \n",
    "            for label in y_true\n",
    "        ]\n",
    "        y_pred_filtered = [\n",
    "            label if label in selected_entities or label == \"O\" else \"O\" \n",
    "            for label in y_pred\n",
    "        ]\n",
    "    else:\n",
    "        y_true_filtered = y_true\n",
    "        y_pred_filtered = y_pred\n",
    "\n",
    "    # Calculate precision, recall, F1-score, and accuracy across only the selected labels\n",
    "    print(classification_report(y_true_filtered, y_pred_filtered, zero_division=0))\n",
    "\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load the trained model\n",
    "output_dir = \"./model/NER_NLP_sl2_it20_dp03\"\n",
    "nlp = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)\n",
    "\n",
    "# Define the selected entities that you want to evaluate (e.g., only 'PERSON' and 'ORG')\n",
    "selected_entities = [\"Skills\", \"Name\"]\n",
    "\n",
    "# Test the model and evaluate it\n",
    "examples = convert_dataturks_to_spacy(\"../testdata.json\", selected_entities)\n",
    "examples = trim_entity_spans(examples)\n",
    "for _, annotations in examples:\n",
    "    annotations['entities'] = keep_longest_entities(annotations['entities'])  # Keep only longest entities\n",
    "# Call the evaluate_model function with the selected entities\n",
    "evaluate_model(nlp, examples, selected_entities)\n",
    "#evaluate_model(nlp, examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/testdata.json\", 'r') as f:\n",
    "    test_data = []\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./model/NER_NLP_sl2_it30\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "output_dir = \"./model/NER_NLP_sl2_it30\"\n",
    "nlp = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Skills', 'Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)'), ('Skills', '• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.')]\n",
      "[('Skills', 'servicenow (1 year), Mainframe (3 years), cobol (3 years), Jcl (3 years), Teradata (3 years)')]\n",
      "[('Skills', 'Database (3 years), SQL (3 years), Sql Dba')]\n",
      "[('Skills', 'AJAX'), ('Skills', 'APACHE'), ('Skills', 'Languages: C, C++, Java (J2EE),\\nWeb Component APIS:: Jdbc, Servlet, JSP.')]\n",
      "[('Skills', 'JAVA (1 year), C++ (Less than 1 year), Hadoop (Less than 1 year), HADOOP (Less than 1 year),\\nCSS (Less than 1 year)'), ('Skills', 'Programming Languages: C, C++, HTML/CSS, Java, Python, Javascript\\n\\nTechnologies: IoT, MySQL, PostgreSQL, D3js, Hadoop and Spark, Gephi')]\n"
     ]
    }
   ],
   "source": [
    "for text in test_data[1:6]:\n",
    "    content = text['content']\n",
    "    #content = ' '.join(content.split())\n",
    "    doc = nlp(content)  # Apply the model to the test text\n",
    "    entities = [(ent.label_, ent.text) for ent in doc.ents if ent.label_ == \"Skills\"]  # Extracting entities and their labels\n",
    "    print(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java\n",
      "• Programming Languages: C, C++, Java, .net, php. • Web Designing: HTML, XML • Operating Systems: Windows […] Windows Server 2003, Linux. • Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql. https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n"
     ]
    }
   ],
   "source": [
    "for text in test_data[:4]:\n",
    "    content = text['content']\n",
    "    content = ' '.join(content.split())\n",
    "    doc = nlp(content)  # Apply the model to the test text\n",
    "    entities = [(ent.label_, ent.text) for ent in doc.ents if ent.label_ == \"Skills\"]  # Extracting entities and their labels\n",
    "    \n",
    "    # Loop over each entity and print it on a new line\n",
    "    for ent_label, skill in entities:\n",
    "        print(skill)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model with real pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Function to extract text from a PDF using pdfminer.six\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Use pdfminer.six's extract_text function to get the text from the PDF\n",
    "        text = extract_text(pdf_path)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to format the extracted text in the same format as testdata.json\n",
    "def format_extracted_text(pdf_text, filename):\n",
    "    return {\"content\": pdf_text, \"annotation\": [{\"label\": [], \"points\": []}]}\n",
    "\n",
    "# Main function to extract text from multiple PDFs in a folder and save to a JSON file\n",
    "def extract_text_from_pdfs_in_folder(folder_path, output_json_path):\n",
    "    extracted_data = []\n",
    "    \n",
    "    with open(output_json_path, 'w') as output_file:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(folder_path, filename)\n",
    "                print(f\"Extracting text from: {pdf_path}\")\n",
    "                \n",
    "                # Extract text from the PDF\n",
    "                pdf_text = extract_text_from_pdf(pdf_path)\n",
    "                \n",
    "                if pdf_text:  # Only process if text was extracted successfully\n",
    "                    formatted_data = format_extracted_text(pdf_text, filename)\n",
    "                    \n",
    "                    # Dump each resume as a single line\n",
    "                    json.dump(formatted_data, output_file, ensure_ascii=False)\n",
    "                    output_file.write(\"\\n\")  # Add a newline after each resume's data\n",
    "    \n",
    "    print(f\"Text extraction completed. Data saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: ./data/Resume_data_pdf/Deemah_Alabdulaali_Resume.pdf\n",
      "Extracting text from: ./data/Resume_data_pdf/Ali Abuharb's CV.pdf\n",
      "Extracting text from: ./data/Resume_data_pdf/Whitmore-resume.pdf\n",
      "Extracting text from: ./data/Resume_data_pdf/resume_juanjosecarin.pdf\n",
      "Text extraction completed. Data saved to ./data/Resume_data_pdf/resume_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extract_text_from_pdfs_in_folder('../Resume_data_pdf', '../data/Resume_data_pdf/resume_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./model/NER_NLP_sl2_it30\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "output_dir = \"../model/NER_NLP_sl2_it30\"\n",
    "nlp = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume 1:\n",
      "[('Name', 'Ali Ibrahim'), ('Skills', 'Technical Skills: \\n\\n•  Deep Learning, Machine Learning, NLP, \\n\\nComputer Vision \\n\\n•  Python \\n\\n•  PySpark \\n\\n• \\n\\nSQL, T-SQL, PL-SQL \\n\\n•  Alteryx \\n\\n•  Data Quality \\n\\n•  Data Engineering \\n\\n•  Prompt Engineering \\n\\n•  HTML5, CSS3 \\n\\n•  Google Analytics \\n\\n•  Minitab \\n\\n•  Anylogic \\n\\n•  Microsoft PowerBi, Tableau \\n\\n•  Back-End Development \\n\\n•  Data Analytical/Calculation Engines \\n\\n• \\n\\nStatistics')]\n",
      "\n",
      "\n",
      "Resume 2:\n",
      "[('Name', 'Jonathan Whitmore'), ('Skills', 'Languages Python, SQL (Impala/Hive), R, LATEX, Bash.')]\n",
      "\n",
      "\n",
      "Resume 3:\n",
      "[('Name', 'Mountain View'), ('Skills', 'Madrid, Spain \\n\\n•  Highest-rated professor in student surveys, in 4 of the 6 terms. \\n•  Increased ratio of students passing the course by 25%. \\n\\nSee juanjocarin.github.io for additional information')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"../data/Resume_data_pdf/resume_data.json\", 'r') as f:\n",
    "    resume_data = []\n",
    "    for line in f:\n",
    "        resume_data.append(json.loads(line))\n",
    "        \n",
    "# Iterate through the resume entries and print the content\n",
    "for idx, entry in enumerate(resume_data):\n",
    "    print(f\"Resume {idx + 1}:\")\n",
    "    #print(entry['content'])  # Print the cleaned resume content\n",
    "    doc = nlp(entry['content'])  # Apply the model to the test text\n",
    "    entities = [(ent.label_, ent.text) for ent in doc.ents]  # Extracting entities and their labels\n",
    "    print(entities)\n",
    "    print(\"\\n\")  # Add a blank line between resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the name, skills, 'Degree' 'Years of Experience' from \n",
    "# ('College Name', 'Companies worked at', 'Degree', 'Designation', 'Email Address', 'Graduation Year',\n",
    "#  'Location', 'Name', 'Skills', 'UNKNOWN', 'Years of Experience')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train NER with pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Angad Waghmare\n",
      "Pune, Maharashtra - Email me on Ind...\" with entities \"[(0, 14, 'Name'), (3111, 3846, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Nida Khan\n",
      "Tech Support Executive - Teleperformance...\" with entities \"[(0, 9, 'Name'), (800, 858, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ramya. P\n",
      "Hyderabad, Telangana - Email me on Indeed...\" with entities \"[(0, 8, 'Name'), (3603, 3612, 'Skills'), (3624, 36...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Priyesh Dubey\n",
      "Azure Developer with 9 Yrs 8 months ...\" with entities \"[(0, 13, 'Name'), (2416, 2493, 'Skills'), (2537, 2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Laya A\n",
      "Cluster HR Manager - Velammal New\n",
      "\n",
      "Chennai,...\" with entities \"[(0, 6, 'Name'), (3760, 4638, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Yogesh Ghatole\n",
      "Engineer / Electrical Supervisor, S...\" with entities \"[(0, 14, 'Name'), (2912, 3288, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Soumya Balan\n",
      "IT SUPPORT\n",
      "\n",
      "Sulthan Bathery, Kerala, ...\" with entities \"[(0, 12, 'Name'), (3913, 4370, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Vineeth Vijayan\n",
      "\"Store Executive\" - Orange City Ho...\" with entities \"[(0, 15, 'Name'), (6994, 7350, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Akshay Dubey\n",
      "Actively looking for opportunity in ....\" with entities \"[(0, 12, 'Name'), (2734, 2846, 'Skills'), (2889, 3...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Suresh Kanagala\n",
      "Architecture SharePoint/Office 365...\" with entities \"[(0, 15, 'Name'), (962, 1095, 'Skills'), (1171, 15...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Mohammed Murtuza\n",
      "Major Incident Manager / Escalati...\" with entities \"[(0, 16, 'Name'), (7924, 8039, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Fenil Francis\n",
      "head of operation and logistics\n",
      "\n",
      "Tri...\" with entities \"[(0, 13, 'Name'), (694, 728, 'Skills'), (774, 897,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Shraddha Achar\n",
      "Mathura, Uttar Pradesh - Email me o...\" with entities \"[(0, 14, 'Name'), (723, 797, 'Skills'), (975, 1020...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Jaspreet Kaur\n",
      "Oceanic Consultants as a HR Executiv...\" with entities \"[(0, 13, 'Name'), (5670, 5780, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Chhaya Prabhale\n",
      "Kharadi, Pune, 411014, IN - Email ...\" with entities \"[(0, 15, 'Name'), (1943, 2050, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ajay Elango\n",
      "Software Engineer\n",
      "\n",
      "Bangalore City, Kar...\" with entities \"[(6930, 7494, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Dilliraja Baskaran\n",
      "Tamil Nadu - Email me on Indeed...\" with entities \"[(0, 17, 'Name'), (314, 349, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Praveen Bhaskar\n",
      "Program Manager (Software Delivery...\" with entities \"[(0, 15, 'Name'), (4459, 4959, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Rupesh Reddy\n",
      "Technology Consultant - EIT Services ...\" with entities \"[(0, 12, 'Name'), (6732, 6848, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"B. Gokul\n",
      "Gokul, Uttar Pradesh - Email me on Indeed...\" with entities \"[(0, 8, 'Name'), (681, 834, 'Skills'), (970, 1002,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Arpit Godha\n",
      "Senior Process Executive\n",
      "\n",
      "Jaipur, Raja...\" with entities \"[(0, 11, 'Name'), (3144, 3495, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Shreyanshu Gupta\n",
      "Software Development Engineer wit...\" with entities \"[(0, 16, 'Name'), (78, 82, 'Skills'), (84, 85, 'Sk...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Kandrapu Reddy\n",
      "Senior Travel Operations (Domestic,...\" with entities \"[(0, 14, 'Name'), (4232, 4330, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Divesh Singh\n",
      "Bengaluru, Karnataka - Email me on In...\" with entities \"[(0, 12, 'Name'), (948, 1180, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Akansha Jain\n",
      "Pune, Maharashtra - Email me on Indee...\" with entities \"[(0, 12, 'Name'), (1263, 1379, 'Skills'), (1502, 1...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Rajeev Kumar\n",
      "Senior Associate Consultant - Infosys...\" with entities \"[(0, 12, 'Name'), (3162, 3674, 'Skills'), (3982, 4...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Saurabh Sandhikar\n",
      "SAURABH SANDHIKAR\n",
      "\n",
      "Hyderabad, Te...\" with entities \"[(0, 17, 'Name'), (1930, 2063, 'Skills'), (2562, 2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ganesh AlalaSundaram\n",
      "A Dev-Test Professional with ...\" with entities \"[(0, 20, 'Name'), (3321, 3376, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Aarti Pimplay\n",
      "Operations Center Shift Manager (OCS...\" with entities \"[(0, 13, 'Name'), (3054, 3363, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Sohan Dhakad\n",
      "Shivpuri, Madhya Pradesh - Email me o...\" with entities \"[(0, 12, 'Name'), (870, 893, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Imgeeyaul Ansari\n",
      "java developer\n",
      "\n",
      "Pune, Maharashtra...\" with entities \"[(0, 16, 'Name'), (1726, 1850, 'Skills'), (1894, 2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Rahul Tayade\n",
      "Global Production Support Lead, - Inf...\" with entities \"[(0, 12, 'Name'), (11131, 11235, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ashish Indoriya\n",
      "Sr. Systems Engineer at Infosys Li...\" with entities \"[(0, 15, 'Name'), (3828, 3931, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Siddhartha Chetri\n",
      "7 years of experience in IT Netw...\" with entities \"[(0, 17, 'Name'), (5471, 5838, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 4011.3768301297264}\n",
      "Starting iteration 1\n",
      "{'ner': 906.5654074766377}\n",
      "Starting iteration 2\n",
      "{'ner': 709.2908992166836}\n",
      "Starting iteration 3\n",
      "{'ner': 617.0802561125992}\n",
      "Starting iteration 4\n",
      "{'ner': 660.3952617280578}\n",
      "Starting iteration 5\n",
      "{'ner': 547.9114828302982}\n",
      "Starting iteration 6\n",
      "{'ner': 498.86874177609633}\n",
      "Starting iteration 7\n",
      "{'ner': 518.0998737591793}\n",
      "Starting iteration 8\n",
      "{'ner': 429.5295563592826}\n",
      "Starting iteration 9\n",
      "{'ner': 449.01633797383744}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "\n",
    "def train_spacy(selected_entities):\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"../data/traindata.json\", selected_entities)\n",
    "    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n",
    "\n",
    "    # Load the pre-trained English model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    # Get the NER pipeline component (it exists in the pre-trained model)\n",
    "    ner = nlp.get_pipe('ner')\n",
    "\n",
    "    # Add only the selected entities to the NER component\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        annotations['entities'] = keep_longest_entities(annotations['entities'])\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # Define a function that returns an iterable of Example objects\n",
    "    def get_examples():\n",
    "        for text, annotations in TRAIN_DATA:\n",
    "            doc = nlp.make_doc(text)\n",
    "            yield Example.from_dict(doc, annotations)\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    \n",
    "    with nlp.disable_pipes(*other_pipes):  # Only train the NER\n",
    "        # Reinitialize the NER model before fine-tuning\n",
    "        optimizer = nlp.resume_training()\n",
    "\n",
    "        # Train for 30 iterations\n",
    "        for itn in range(10):  # Increase the number of iterations as necessary\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    drop=0.1,  # Adjust dropout\n",
    "                    sgd=optimizer,\n",
    "                    losses=losses,\n",
    "                )\n",
    "            print(losses)\n",
    "\n",
    "    return losses, nlp\n",
    "\n",
    "# Train the model\n",
    "selected_entities = [\"Skills\", \"Name\"]\n",
    "losses, nlp = train_spacy(selected_entities)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./model/NER_NLP_pretrain_ts\n"
     ]
    }
   ],
   "source": [
    "# spaCy model and it's stored in `nlp`\n",
    "output_dir = \"./model/NER_NLP_pretrain_ts\"\n",
    "# Save the trained model to the output directory\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
