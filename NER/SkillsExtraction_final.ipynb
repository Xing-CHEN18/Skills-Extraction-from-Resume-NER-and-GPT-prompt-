{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import spacy\n",
    "from spacy.training import Example\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from spacy.tokens import Span\n",
    "import re\n",
    "#from spacy.training import Scorer\n",
    "from spacy.training import Example\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, classification_report\n",
    "import fitz  # PyMuPDF\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with selected entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "{'ner': 5214.083728252095}\n",
      "Starting iteration 1\n",
      "{'ner': 791.2761714185349}\n",
      "Starting iteration 2\n",
      "{'ner': 727.1208322038265}\n",
      "Starting iteration 3\n",
      "{'ner': 742.1385025036657}\n",
      "Starting iteration 4\n",
      "{'ner': 677.9047111781462}\n",
      "Starting iteration 5\n",
      "{'ner': 694.0711231891801}\n",
      "Starting iteration 6\n",
      "{'ner': 753.6272058137095}\n",
      "Starting iteration 7\n",
      "{'ner': 648.5127004660479}\n",
      "Starting iteration 8\n",
      "{'ner': 614.0949430490323}\n",
      "Starting iteration 9\n",
      "{'ner': 593.9946133920737}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.training import Example\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import logging\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            if valid_start < valid_end:\n",
    "                valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath, selected_entities):\n",
    "    try:\n",
    "        training_data = []\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content']\n",
    "            entities = []\n",
    "            for annotation in data['annotation']:\n",
    "                point = annotation['points'][0]\n",
    "                labels = annotation['label']\n",
    "                if not isinstance(labels, list):\n",
    "                    labels = [labels]\n",
    "\n",
    "                for label in labels:\n",
    "                    # Filter out only \"Skills\" and \"Name\" labels\n",
    "                    if label in selected_entities:\n",
    "                        entities.append((point['start'], point['end'] + 1, label))\n",
    "\n",
    "            training_data.append((text, {\"entities\": entities}))\n",
    "\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def keep_longest_entities(entities):\n",
    "    \"\"\"Remove overlapping entities by keeping only the longest one\"\"\"\n",
    "    entities = sorted(entities, key=lambda x: (x[0], -(x[1] - x[0])))\n",
    "\n",
    "    non_overlapping_entities = []\n",
    "    \n",
    "    for i, (start, end, label) in enumerate(entities):\n",
    "        if i == 0:\n",
    "            non_overlapping_entities.append((start, end, label))\n",
    "        else:\n",
    "            prev_start, prev_end, prev_label = non_overlapping_entities[-1]\n",
    "            if start >= prev_end:\n",
    "                non_overlapping_entities.append((start, end, label))\n",
    "            else:\n",
    "                if (end - start) > (prev_end - prev_start):\n",
    "                    non_overlapping_entities[-1] = (start, end, label)\n",
    "\n",
    "    return non_overlapping_entities\n",
    "\n",
    "\n",
    "def train_spacy(selected_entities):\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"../data/traindata.json\",selected_entities)\n",
    "    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    \n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner', last=True)\n",
    "\n",
    "    # Add only \"Skills\" and \"Name\" labels to the NER component\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        annotations['entities'] = keep_longest_entities(annotations['entities'])\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])  # Only adds \"Skills\" and \"Name\" labels\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # Only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(10):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update(\n",
    "                    [example],\n",
    "                    drop=0.3,\n",
    "                    sgd=optimizer,\n",
    "                    losses=losses\n",
    "                )\n",
    "            print(losses)\n",
    "\n",
    "    return losses, nlp\n",
    "\n",
    "# Train the model\n",
    "#'College Name', 'Companies worked at', 'Degree', 'Designation', 'Email Address', 'Graduation Year', 'Location', 'Name', 'Skills', 'UNKNOWN', 'Years of Experience'\n",
    "selected_entities = [\"Skills\", \"Name\"]\n",
    "losses, nlp = train_spacy(selected_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline components: ['ner']\n",
      "NER labels: ('Name', 'Skills')\n",
      "NER configuration: {'moves': None, 'update_with_oracle_cut_size': 100, 'multitasks': [], 'min_action_freq': 1, 'learn_tokens': False, 'beam_width': 1, 'beam_density': 0.0, 'beam_update_prob': 0.0, 'incorrect_spans_key': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"Pipeline components:\", nlp.pipe_names)\n",
    "# Accessing the NER component parameters\n",
    "ner = nlp.get_pipe('ner')\n",
    "print(\"NER labels:\", ner.labels)\n",
    "\n",
    "# You can also inspect the model's config settings\n",
    "print(\"NER configuration:\", ner.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paths': {'train': None, 'dev': None, 'vectors': None, 'init_tok2vec': None},\n",
       " 'system': {'seed': 0, 'gpu_allocator': None},\n",
       " 'nlp': {'lang': 'en',\n",
       "  'pipeline': ['ner'],\n",
       "  'disabled': [],\n",
       "  'before_creation': None,\n",
       "  'after_creation': None,\n",
       "  'after_pipeline_creation': None,\n",
       "  'batch_size': 1000,\n",
       "  'tokenizer': {'@tokenizers': 'spacy.Tokenizer.v1'},\n",
       "  'vectors': {'@vectors': 'spacy.Vectors.v1'}},\n",
       " 'components': {'ner': {'factory': 'ner',\n",
       "   'incorrect_spans_key': None,\n",
       "   'model': {'@architectures': 'spacy.TransitionBasedParser.v2',\n",
       "    'state_type': 'ner',\n",
       "    'extra_state_tokens': False,\n",
       "    'hidden_width': 64,\n",
       "    'maxout_pieces': 2,\n",
       "    'use_upper': True,\n",
       "    'tok2vec': {'@architectures': 'spacy.HashEmbedCNN.v2',\n",
       "     'pretrained_vectors': None,\n",
       "     'width': 96,\n",
       "     'depth': 4,\n",
       "     'embed_size': 2000,\n",
       "     'window_size': 1,\n",
       "     'maxout_pieces': 3,\n",
       "     'subword_features': True},\n",
       "    'nO': None},\n",
       "   'moves': None,\n",
       "   'scorer': {'@scorers': 'spacy.ner_scorer.v1'},\n",
       "   'update_with_oracle_cut_size': 100}},\n",
       " 'corpora': {'dev': {'@readers': 'spacy.Corpus.v1',\n",
       "   'path': '${paths.dev}',\n",
       "   'gold_preproc': False,\n",
       "   'max_length': 0,\n",
       "   'limit': 0,\n",
       "   'augmenter': None},\n",
       "  'train': {'@readers': 'spacy.Corpus.v1',\n",
       "   'path': '${paths.train}',\n",
       "   'gold_preproc': False,\n",
       "   'max_length': 0,\n",
       "   'limit': 0,\n",
       "   'augmenter': None}},\n",
       " 'training': {'seed': '${system.seed}',\n",
       "  'gpu_allocator': '${system.gpu_allocator}',\n",
       "  'dropout': 0.1,\n",
       "  'accumulate_gradient': 1,\n",
       "  'patience': 1600,\n",
       "  'max_epochs': 0,\n",
       "  'max_steps': 20000,\n",
       "  'eval_frequency': 200,\n",
       "  'score_weights': {'ents_f': 1.0,\n",
       "   'ents_p': 0.0,\n",
       "   'ents_r': 0.0,\n",
       "   'ents_per_type': None},\n",
       "  'frozen_components': [],\n",
       "  'annotating_components': [],\n",
       "  'dev_corpus': 'corpora.dev',\n",
       "  'train_corpus': 'corpora.train',\n",
       "  'before_to_disk': None,\n",
       "  'before_update': None,\n",
       "  'batcher': {'@batchers': 'spacy.batch_by_words.v1',\n",
       "   'discard_oversize': False,\n",
       "   'tolerance': 0.2,\n",
       "   'size': {'@schedules': 'compounding.v1',\n",
       "    'start': 100,\n",
       "    'stop': 1000,\n",
       "    'compound': 1.001,\n",
       "    't': 0.0},\n",
       "   'get_length': None},\n",
       "  'logger': {'@loggers': 'spacy.ConsoleLogger.v1', 'progress_bar': False},\n",
       "  'optimizer': {'@optimizers': 'Adam.v1',\n",
       "   'beta1': 0.9,\n",
       "   'beta2': 0.999,\n",
       "   'L2_is_weight_decay': True,\n",
       "   'L2': 0.01,\n",
       "   'grad_clip': 1.0,\n",
       "   'use_averages': False,\n",
       "   'eps': 1e-08,\n",
       "   'learn_rate': 0.001}},\n",
       " 'pretraining': {},\n",
       " 'initialize': {'vectors': '${paths.vectors}',\n",
       "  'init_tok2vec': '${paths.init_tok2vec}',\n",
       "  'vocab_data': None,\n",
       "  'lookups': None,\n",
       "  'tokenizer': {},\n",
       "  'components': {},\n",
       "  'before_init': None,\n",
       "  'after_init': None}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ../model/NER_NLP_it10\n"
     ]
    }
   ],
   "source": [
    "# save the model for n interation\n",
    "# spaCy model and it's stored in `nlp`\n",
    "output_dir = \"../model/NER_NLP_it10\"\n",
    "# Save the trained model to the output directory\n",
    "nlp.to_disk(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER training with validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to split a separate validation dataset train with the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to split training data into train and validation sets\n",
    "def split_data(data, test_size=0.2):\n",
    "    train_data, val_data = train_test_split(data, test_size=test_size, random_state=42)\n",
    "    return train_data, val_data\n",
    "\n",
    "# Function to calculate validation loss\n",
    "def evaluate_on_validation(nlp, val_data):\n",
    "    val_losses = {}\n",
    "    for text, annotations in val_data:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, annotations)\n",
    "        nlp.update([example], drop=0.0, losses=val_losses)\n",
    "    return val_losses\n",
    "\n",
    "def train_spacy():\n",
    "    TRAIN_DATA = convert_dataturks_to_spacy(\"../data/traindata.json\")\n",
    "    TRAIN_DATA = trim_entity_spans(TRAIN_DATA)\n",
    "\n",
    "    # Split data into training and validation sets (80% train, 20% validation)\n",
    "    train_data, val_data = split_data(TRAIN_DATA, test_size=0.1)\n",
    "\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.add_pipe('ner', last=True)\n",
    "\n",
    "    # Add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "        annotations['entities'] = keep_longest_entities(annotations['entities'])  # Keep only longest entities\n",
    "        for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(50):\n",
    "            print(f\"Starting iteration {itn}\")\n",
    "            random.shuffle(train_data)\n",
    "            train_losses = {}\n",
    "            \n",
    "            # Training phase\n",
    "            for text, annotations in train_data:\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = Example.from_dict(doc, annotations)\n",
    "                nlp.update([example], drop=0.2, sgd=optimizer, losses=train_losses)\n",
    "\n",
    "            # Validation phase (no training, just forward pass)\n",
    "            val_losses = evaluate_on_validation(nlp, val_data)\n",
    "            \n",
    "            # Print losses for both training and validation sets\n",
    "            print(f\"Iteration {itn}:\")\n",
    "            print(f\"Training Loss: {train_losses}\")\n",
    "            print(f\"Validation Loss: {val_losses}\")\n",
    "\n",
    "    return nlp, val_losses, train_losses\n",
    "\n",
    "# Train the model and return it\n",
    "trained_nlp, val_losses, train_losses = train_spacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../model/NER_NLP_it10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xing/work/StillsExtraction/my_env/lib/python3.10/site-packages/spacy/training/iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Ananya Chavan\n",
      "lecturer - oracle tutorials\n",
      "\n",
      "Mumbai,...\" with entities \"[(0, 13, 'Name'), (973, 1703, 'Skills')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Name       0.95      0.97      0.96        39\n",
      "           O       0.97      0.99      0.98     12285\n",
      "      Skills       0.86      0.64      0.73      1184\n",
      "\n",
      "    accuracy                           0.96     13508\n",
      "   macro avg       0.92      0.87      0.89     13508\n",
      "weighted avg       0.96      0.96      0.96     13508\n",
      "\n",
      "Accuracy: 95.88%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(nlp, test_data, selected_entities=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for text, annotations in test_data:\n",
    "        doc = nlp(text)  # Apply the model to the test text\n",
    "        example = Example.from_dict(nlp.make_doc(text), annotations)  # Create example using ground truth\n",
    "        \n",
    "        # Ensure the example's NER alignment is valid\n",
    "        aligned_ner = example.get_aligned_ner()\n",
    "        \n",
    "        if aligned_ner is None:\n",
    "            print(f\"Warning: NER alignment failed for text: {text}\")\n",
    "            continue\n",
    "        \n",
    "        # Create a list of token-based true labels for the text\n",
    "        true_labels = [\"O\"] * len(doc)  # Initialize with \"O\" (no entity)\n",
    "        \n",
    "        for start, end, label in annotations[\"entities\"]:\n",
    "            for token in doc:\n",
    "                if token.idx >= start and token.idx + len(token) <= end:\n",
    "                    true_labels[token.i] = label\n",
    "        \n",
    "        # Create a list of token-based predicted labels\n",
    "        pred_labels = [\"O\"] * len(doc)\n",
    "        for ent in doc.ents:\n",
    "            for token in doc:\n",
    "                if token.idx >= ent.start_char and token.idx + len(token) <= ent.end_char:\n",
    "                    pred_labels[token.i] = ent.label_\n",
    "        \n",
    "        # Add the true and predicted labels to the global lists\n",
    "        y_true.extend(true_labels)\n",
    "        y_pred.extend(pred_labels)\n",
    "\n",
    "    # Filter y_true and y_pred to only include selected entities\n",
    "    if selected_entities is not None:\n",
    "        y_true_filtered = [\n",
    "            label if label in selected_entities or label == \"O\" else \"O\" \n",
    "            for label in y_true\n",
    "        ]\n",
    "        y_pred_filtered = [\n",
    "            label if label in selected_entities or label == \"O\" else \"O\" \n",
    "            for label in y_pred\n",
    "        ]\n",
    "    else:\n",
    "        y_true_filtered = y_true\n",
    "        y_pred_filtered = y_pred\n",
    "\n",
    "    # Calculate precision, recall, F1-score, and accuracy across only the selected labels\n",
    "    print(classification_report(y_true_filtered, y_pred_filtered, zero_division=0))\n",
    "\n",
    "    accuracy = accuracy_score(y_true_filtered, y_pred_filtered)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Load the trained model\n",
    "output_dir = \"../model/NER_NLP_it10\"\n",
    "nlp = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)\n",
    "\n",
    "# Define the selected entities that you want to evaluate (e.g., only 'PERSON' and 'ORG')\n",
    "selected_entities = [\"Skills\", \"Name\"]\n",
    "\n",
    "# Test the model and evaluate it\n",
    "examples = convert_dataturks_to_spacy(\"../data/testdata.json\", selected_entities)\n",
    "examples = trim_entity_spans(examples)\n",
    "for _, annotations in examples:\n",
    "    annotations['entities'] = keep_longest_entities(annotations['entities'])  # Keep only longest entities\n",
    "# Call the evaluate_model function with the selected entities\n",
    "evaluate_model(nlp, examples, selected_entities)\n",
    "#evaluate_model(nlp, examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../model/NER_NLP_it10\n",
      "[('Skills', 'Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)')]\n",
      "[('Skills', 'servicenow (1 year), Mainframe (3 years), cobol (3 years), Jcl (3 years), Teradata (3 years)')]\n",
      "[('Skills', 'Database (3 years), SQL (3 years), Sql Dba')]\n",
      "[('Skills', 'SEARCH ENGINE MARKETING (2 years), SEM (2 years), ACCESS (Less than 1 year), AJAX (Less\\nthan 1 year), APACHE (Less than 1 year)')]\n",
      "[('Skills', 'JAVA (1 year), C++ (Less than 1 year), Hadoop (Less than 1 year), HADOOP (Less than 1 year),\\nCSS (Less than 1 year)')]\n"
     ]
    }
   ],
   "source": [
    "# show the testing performance with the testdata.json\n",
    "with open(\"../data/testdata.json\", 'r') as f:\n",
    "    test_data = []\n",
    "    for line in f:\n",
    "        test_data.append(json.loads(line))\n",
    "# load the trained model\n",
    "output_dir = \"../model/NER_NLP_it10\"\n",
    "nlp = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)\n",
    "\n",
    "for text in test_data[1:6]:\n",
    "    content = text['content']\n",
    "    #content = ' '.join(content.split())\n",
    "    doc = nlp(content)  # Apply the model to the test text\n",
    "    entities = [(ent.label_, ent.text) for ent in doc.ents if ent.label_ == \"Skills\"]  # Extracting entities and their labels\n",
    "    print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model with real pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Function to extract text from a PDF using pdfminer.six\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        # Use pdfminer.six's extract_text function to get the text from the PDF\n",
    "        text = extract_text(pdf_path)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to format the extracted text in the same format as testdata.json\n",
    "def format_extracted_text(pdf_text, filename):\n",
    "    return {\"content\": pdf_text, \"annotation\": [{\"label\": [], \"points\": []}]}\n",
    "\n",
    "# Main function to extract text from multiple PDFs in a folder and save to a JSON file\n",
    "def extract_text_from_pdfs_in_folder(folder_path, output_json_path):\n",
    "    extracted_data = []\n",
    "    \n",
    "    with open(output_json_path, 'w') as output_file:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".pdf\"):\n",
    "                pdf_path = os.path.join(folder_path, filename)\n",
    "                print(f\"Extracting text from: {pdf_path}\")\n",
    "                \n",
    "                # Extract text from the PDF\n",
    "                pdf_text = extract_text_from_pdf(pdf_path)\n",
    "                \n",
    "                if pdf_text:  # Only process if text was extracted successfully\n",
    "                    formatted_data = format_extracted_text(pdf_text, filename)\n",
    "                    \n",
    "                    # Dump each resume as a single line\n",
    "                    json.dump(formatted_data, output_file, ensure_ascii=False)\n",
    "                    output_file.write(\"\\n\")  # Add a newline after each resume's data\n",
    "    \n",
    "    print(f\"Text extraction completed. Data saved to {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting text from: ../data/Resume_data_pdf/Deemah_Alabdulaali_Resume.pdf\n",
      "Extracting text from: ../data/Resume_data_pdf/Ali Abuharb's CV.pdf\n",
      "Extracting text from: ../data/Resume_data_pdf/Whitmore-resume.pdf\n",
      "Extracting text from: ../data/Resume_data_pdf/resume_juanjosecarin.pdf\n",
      "Text extraction completed. Data saved to ../data/Resume_data_pdf/resume_data.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "extract_text_from_pdfs_in_folder('../data/Resume_data_pdf', '../data/Resume_data_pdf/resume_data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ../model/NER_NLP_it10\n",
      "Resume 1:\n",
      "[('Name', 'Ali Ibrahim'), ('Skills', 'Technical Skills: \\n\\n•  Deep Learning, Machine Learning, NLP, \\n\\nComputer Vision \\n\\n•  Python \\n\\n•  PySpark \\n\\n• \\n\\nSQL, T-SQL, PL-SQL \\n\\n•  Alteryx \\n\\n•  Data Quality \\n\\n•  Data Engineering \\n\\n•  Prompt Engineering \\n\\n•  HTML5, CSS3 \\n\\n•  Google Analytics \\n\\n•  Minitab \\n\\n•  Anylogic \\n\\n•  Microsoft PowerBi, Tableau \\n\\n•  Back-End Development \\n\\n•  Data Analytical/Calculation Engines \\n\\n• \\n\\nStatistics')]\n",
      "\n",
      "\n",
      "Resume 2:\n",
      "[('Name', 'Jonathan Whitmore'), ('Skills', 'Languages Python, SQL (Impala/Hive), R, LATEX, Bash.')]\n",
      "\n",
      "\n",
      "Resume 3:\n",
      "[('Name', 'Mountain View')]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load the trained model\n",
    "output_dir = \"../model/NER_NLP_it10\"\n",
    "nlp = spacy.load(output_dir)\n",
    "print(\"Model loaded from\", output_dir)\n",
    "with open(\"../data/Resume_data_pdf/resume_data.json\", 'r') as f:\n",
    "    resume_data = []\n",
    "    for line in f:\n",
    "        resume_data.append(json.loads(line))\n",
    "        \n",
    "# Iterate through the resume entries and print the content\n",
    "for idx, entry in enumerate(resume_data):\n",
    "    print(f\"Resume {idx + 1}:\")\n",
    "    #print(entry['content'])  # Print the cleaned resume content\n",
    "    doc = nlp(entry['content'])  # Apply the model to the test text\n",
    "    entities = [(ent.label_, ent.text) for ent in doc.ents]  # Extracting entities and their labels\n",
    "    print(entities)\n",
    "    print(\"\\n\")  # Add a blank line between resumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the name, skills, 'Degree' 'Years of Experience' from \n",
    "# ('College Name', 'Companies worked at', 'Degree', 'Designation', 'Email Address', 'Graduation Year',\n",
    "#  'Location', 'Name', 'Skills', 'UNKNOWN', 'Years of Experience')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
